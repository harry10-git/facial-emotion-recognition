{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYtmK7gkehUI"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle/"
      ],
      "metadata": {
        "id": "1zfoZYbtfjsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d jonathanoheix/face-expression-recognition-dataset"
      ],
      "metadata": {
        "id": "dXJ4gwMQfJAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip face-expression-recognition-dataset.zip"
      ],
      "metadata": {
        "id": "lDKVEIzlfdY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Define the dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, transform=None, string=\"train\"):\n",
        "        self.imgs_path = \"images/\" + string + \"/\"\n",
        "        file_list = glob.glob(self.imgs_path + \"*\")\n",
        "        self.data = []\n",
        "        for class_path in file_list:\n",
        "            class_name = os.path.basename(class_path)\n",
        "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
        "                self.data.append([img_path, class_name])\n",
        "        self.class_map = {\"angry\": 0, \"disgust\": 1, \"fear\": 2, \"happy\": 3, \"neutral\": 4, \"sad\": 5, \"surprise\": 6}\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_name = self.data[idx]\n",
        "        img = Image.open(img_path)\n",
        "        class_id = self.class_map[class_name]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, class_id\n",
        "\n",
        "# Define transforms for preprocessing the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize images\n",
        "])\n",
        "\n",
        "# Load train and validation datasets using MyDataset class\n",
        "train_dataset = MyDataset(transform=transform, string=\"train\")\n",
        "val_dataset = MyDataset(transform=transform, string=\"validation\")\n",
        "\n",
        "# Create DataLoader for batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)  # Adjusted input size after flattening\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"Input shape:\", x.shape)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        #print(\"Shape after conv1:\", x.shape)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        #print(\"Shape after maxpool1:\", x.shape)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        #print(\"Shape after conv2:\", x.shape)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        #print(\"Shape after maxpool2:\", x.shape)\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        #print(\"Shape after conv3:\", x.shape)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        #print(\"Shape after maxpool3:\", x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(\"Shape after flattening:\", x.shape)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        #print(\"Shape after fc1:\", x.shape)\n",
        "        x = self.fc2(x)\n",
        "        #print(\"Shape after fc2:\", x.shape)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmotionClassifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Evaluation on validation set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {(correct / total) * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "QoZXvdtvfuxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Take 5 random images from the validation dataset\n",
        "random_indices = random.sample(range(len(val_dataset)), 2)\n",
        "random_images = [val_dataset[i][0] for i in random_indices]\n",
        "random_labels = [val_dataset[i][1] for i in random_indices]\n",
        "\n",
        "# Predict emotions for the random images\n",
        "with torch.no_grad():\n",
        "    for i, image in enumerate(random_images):\n",
        "        image = image.unsqueeze(0)  # Add batch dimension\n",
        "        image = image.to(device)     # Move image to GPU if available\n",
        "        output = model(image)\n",
        "        _, predicted_label = torch.max(output, 1)\n",
        "        predicted_label = predicted_label.item()\n",
        "\n",
        "        # Map predicted label to emotion\n",
        "        emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "        predicted_emotion = emotions[predicted_label]\n",
        "\n",
        "        # Get actual label (emotion)\n",
        "        actual_label = emotions[random_labels[i]]\n",
        "\n",
        "        # Display the image and labels\n",
        "        plt.imshow(random_images[i].permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "        plt.title(f\"Predicted: {predicted_emotion}\\nActual: {actual_label}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "ER7onjSlf2xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout and Reg"
      ],
      "metadata": {
        "id": "6_DqT3iciOOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Define the dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, transform=None, string=\"train\"):\n",
        "        self.imgs_path = \"images/\" + string + \"/\"\n",
        "        file_list = glob.glob(self.imgs_path + \"*\")\n",
        "        self.data = []\n",
        "        for class_path in file_list:\n",
        "            class_name = os.path.basename(class_path)\n",
        "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
        "                self.data.append([img_path, class_name])\n",
        "        self.class_map = {\"angry\": 0, \"disgust\": 1, \"fear\": 2, \"happy\": 3, \"neutral\": 4, \"sad\": 5, \"surprise\": 6}\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_name = self.data[idx]\n",
        "        img = Image.open(img_path)\n",
        "        class_id = self.class_map[class_name]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, class_id\n",
        "\n",
        "# Define transforms for preprocessing the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize images\n",
        "])\n",
        "\n",
        "# Load train and validation datasets using MyDataset class\n",
        "train_dataset = MyDataset(transform=transform, string=\"train\")\n",
        "val_dataset = MyDataset(transform=transform, string=\"validation\")\n",
        "\n",
        "# Create DataLoader for batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout2 = nn.Dropout2d(0.25)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout3 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)  # Adjusted input size after flattening\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmotionClassifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)  # Apply weight decay\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Compute training accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "    # Print training loss and accuracy\n",
        "    train_loss = running_loss/len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "    print(f\"Epoch: [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"\\tTraining Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item()\n",
        "            _, predicted_val = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted_val == labels).sum().item()\n",
        "\n",
        "    # Print validation loss and accuracy\n",
        "    val_loss = val_running_loss/len(val_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "    print(f\"\\tValidation Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPP942P5g5T9",
        "outputId": "68a946b0-8560-43c1-e332-05a44ba08f19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/10]\n",
            "\tTraining Loss: 1.6585 | Accuracy: 33.65%\n",
            "\tValidation Loss: 1.4693 | Accuracy: 43.55%\n",
            "Epoch: [2/10]\n",
            "\tTraining Loss: 1.4407 | Accuracy: 44.37%\n",
            "\tValidation Loss: 1.3391 | Accuracy: 48.50%\n",
            "Epoch: [3/10]\n",
            "\tTraining Loss: 1.3517 | Accuracy: 48.41%\n",
            "\tValidation Loss: 1.2858 | Accuracy: 51.51%\n",
            "Epoch: [4/10]\n",
            "\tTraining Loss: 1.2903 | Accuracy: 50.67%\n",
            "\tValidation Loss: 1.2402 | Accuracy: 52.76%\n",
            "Epoch: [5/10]\n",
            "\tTraining Loss: 1.2441 | Accuracy: 52.52%\n",
            "\tValidation Loss: 1.2115 | Accuracy: 54.50%\n",
            "Epoch: [6/10]\n",
            "\tTraining Loss: 1.2094 | Accuracy: 53.77%\n",
            "\tValidation Loss: 1.2167 | Accuracy: 53.64%\n",
            "Epoch: [7/10]\n",
            "\tTraining Loss: 1.1720 | Accuracy: 55.37%\n",
            "\tValidation Loss: 1.1836 | Accuracy: 55.51%\n",
            "Epoch: [8/10]\n",
            "\tTraining Loss: 1.1446 | Accuracy: 56.68%\n",
            "\tValidation Loss: 1.1756 | Accuracy: 56.52%\n",
            "Epoch: [9/10]\n",
            "\tTraining Loss: 1.1144 | Accuracy: 57.97%\n",
            "\tValidation Loss: 1.1632 | Accuracy: 56.52%\n",
            "Epoch: [10/10]\n",
            "\tTraining Loss: 1.0878 | Accuracy: 58.56%\n",
            "\tValidation Loss: 1.1630 | Accuracy: 56.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VpwHA3niKZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Early Stopping**"
      ],
      "metadata": {
        "id": "W27FKHzijCnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Define the dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, transform=None, string=\"train\"):\n",
        "        self.imgs_path = \"images/\" + string + \"/\"\n",
        "        file_list = glob.glob(self.imgs_path + \"*\")\n",
        "        self.data = []\n",
        "        for class_path in file_list:\n",
        "            class_name = os.path.basename(class_path)\n",
        "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
        "                self.data.append([img_path, class_name])\n",
        "        self.class_map = {\"angry\": 0, \"disgust\": 1, \"fear\": 2, \"happy\": 3, \"neutral\": 4, \"sad\": 5, \"surprise\": 6}\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_name = self.data[idx]\n",
        "        img = Image.open(img_path)\n",
        "        class_id = self.class_map[class_name]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, class_id\n",
        "\n",
        "# Define transforms for preprocessing the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert to grayscale\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
        "])\n",
        "\n",
        "# Load train and validation datasets using MyDataset class\n",
        "train_dataset = MyDataset(transform=transform, string=\"train\")\n",
        "val_dataset = MyDataset(transform=transform, string=\"validation\")\n",
        "\n",
        "# Create DataLoader for batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout2 = nn.Dropout2d(0.25)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout3 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)  # Adjusted input size after flattening\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmotionClassifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)  # Apply weight decay\n",
        "\n",
        "# Training loop\n",
        "# Training loop with early stopping\n",
        "num_epochs = 30\n",
        "patience = 3\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item()\n",
        "            _, predicted_val = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted_val == labels).sum().item()\n",
        "\n",
        "    val_loss = val_running_loss / len(val_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "\n",
        "    print(f\"Epoch: [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"\\tTraining Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"\\tValidation Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "    else:\n",
        "        if epoch - best_epoch >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3dI-BaJscTW",
        "outputId": "d95d2708-0fd0-425a-c033-7ed5008ee1b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/30]\n",
            "\tTraining Loss: 1.6363 | Accuracy: 35.06%\n",
            "\tValidation Loss: 1.4665 | Accuracy: 43.76%\n",
            "Epoch: [2/30]\n",
            "\tTraining Loss: 1.4160 | Accuracy: 45.59%\n",
            "\tValidation Loss: 1.3182 | Accuracy: 49.42%\n",
            "Epoch: [3/30]\n",
            "\tTraining Loss: 1.3266 | Accuracy: 48.98%\n",
            "\tValidation Loss: 1.2628 | Accuracy: 52.14%\n",
            "Epoch: [4/30]\n",
            "\tTraining Loss: 1.2688 | Accuracy: 51.21%\n",
            "\tValidation Loss: 1.2234 | Accuracy: 53.34%\n",
            "Epoch: [5/30]\n",
            "\tTraining Loss: 1.2232 | Accuracy: 53.53%\n",
            "\tValidation Loss: 1.1850 | Accuracy: 55.56%\n",
            "Epoch: [6/30]\n",
            "\tTraining Loss: 1.1897 | Accuracy: 54.98%\n",
            "\tValidation Loss: 1.1746 | Accuracy: 55.35%\n",
            "Epoch: [7/30]\n",
            "\tTraining Loss: 1.1526 | Accuracy: 56.24%\n",
            "\tValidation Loss: 1.1766 | Accuracy: 55.43%\n",
            "Epoch: [8/30]\n",
            "\tTraining Loss: 1.1183 | Accuracy: 57.67%\n",
            "\tValidation Loss: 1.1807 | Accuracy: 55.38%\n",
            "Epoch: [9/30]\n",
            "\tTraining Loss: 1.0924 | Accuracy: 58.84%\n",
            "\tValidation Loss: 1.1588 | Accuracy: 56.69%\n",
            "Epoch: [10/30]\n",
            "\tTraining Loss: 1.0609 | Accuracy: 60.17%\n",
            "\tValidation Loss: 1.1625 | Accuracy: 56.72%\n",
            "Epoch: [11/30]\n",
            "\tTraining Loss: 1.0400 | Accuracy: 60.97%\n",
            "\tValidation Loss: 1.1716 | Accuracy: 56.34%\n",
            "Epoch: [12/30]\n",
            "\tTraining Loss: 1.0154 | Accuracy: 61.83%\n",
            "\tValidation Loss: 1.1470 | Accuracy: 57.08%\n",
            "Epoch: [13/30]\n",
            "\tTraining Loss: 0.9937 | Accuracy: 62.77%\n",
            "\tValidation Loss: 1.1855 | Accuracy: 56.76%\n",
            "Epoch: [14/30]\n",
            "\tTraining Loss: 0.9674 | Accuracy: 63.61%\n",
            "\tValidation Loss: 1.1628 | Accuracy: 56.69%\n",
            "Epoch: [15/30]\n",
            "\tTraining Loss: 0.9446 | Accuracy: 64.56%\n",
            "\tValidation Loss: 1.1669 | Accuracy: 57.10%\n",
            "Early stopping at epoch 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_OtQfXbXscrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Define the dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, transform=None, string=\"train\"):\n",
        "        self.imgs_path = \"images/\" + string + \"/\"\n",
        "        file_list = glob.glob(self.imgs_path + \"*\")\n",
        "        self.data = []\n",
        "        for class_path in file_list:\n",
        "            class_name = os.path.basename(class_path)\n",
        "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
        "                self.data.append([img_path, class_name])\n",
        "        self.class_map = {\"angry\": 0, \"disgust\": 1, \"fear\": 2, \"happy\": 3, \"neutral\": 4, \"sad\": 5, \"surprise\": 6}\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_name = self.data[idx]\n",
        "        img = Image.open(img_path)\n",
        "        class_id = self.class_map[class_name]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, class_id\n",
        "\n",
        "# Define transforms for preprocessing the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize images\n",
        "])\n",
        "\n",
        "# Load train and validation datasets using MyDataset class\n",
        "train_dataset = MyDataset(transform=transform, string=\"train\")\n",
        "val_dataset = MyDataset(transform=transform, string=\"validation\")\n",
        "\n",
        "# Create DataLoader for batching and shuffling\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(EmotionClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout2 = nn.Dropout2d(0.25)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout3 = nn.Dropout2d(0.25)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 512)  # Adjusted input size after flattening\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmotionClassifier().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)  # Apply weight decay\n",
        "\n",
        "# Define early stopping parameters\n",
        "patience = 3\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = (correct / total) * 100\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Implement early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}')\n",
        "            break\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "njCuJvE3jJlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Take 5 random images from the validation dataset\n",
        "random_indices = random.sample(range(len(val_dataset)), 2)\n",
        "random_images = [val_dataset[i][0] for i in random_indices]\n",
        "random_labels = [val_dataset[i][1] for i in random_indices]\n",
        "\n",
        "# Predict emotions for the random images\n",
        "with torch.no_grad():\n",
        "    for i, image in enumerate(random_images):\n",
        "        image = image.unsqueeze(0)  # Add batch dimension\n",
        "        image = image.to(device)     # Move image to GPU if available\n",
        "        output = model(image)\n",
        "        _, predicted_label = torch.max(output, 1)\n",
        "        predicted_label = predicted_label.item()\n",
        "\n",
        "        # Map predicted label to emotion\n",
        "        emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "        predicted_emotion = emotions[predicted_label]\n",
        "\n",
        "        # Get actual label (emotion)\n",
        "        actual_label = emotions[random_labels[i]]\n",
        "\n",
        "        # Display the image and labels\n",
        "        plt.imshow(random_images[i].permute(1, 2, 0).numpy().squeeze(), cmap='gray')\n",
        "        plt.title(f\"Predicted: {predicted_emotion}\\nActual: {actual_label}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "AUblnK4AjMnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'emotion_classifier_model.pth')"
      ],
      "metadata": {
        "id": "C05HCA6nnm9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = EmotionClassifier().to(device)\n",
        "loaded_model.load_state_dict(torch.load('emotion_classifier_model.pth'))\n",
        "loaded_model.eval()\n",
        "\n",
        "# Preprocess the new image\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "image_path = '/content/IMG_2131.JPG'\n",
        "image = Image.open(image_path)\n",
        "image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "with torch.no_grad():\n",
        "    output = loaded_model(image_tensor)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
        "    predicted_emotion = emotions[predicted.item()]\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image)\n",
        "plt.title(f'Predicted Emotion: {predicted_emotion}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TCpBatHXn9D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ff92e6dTpnYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}