{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b46b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "epochs = 7\n",
    "batch_size_train = 32\n",
    "batch_size_test = 64\n",
    "lr = 0.001\n",
    "weight_decay = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),  # Convert to RGB\n",
    "    T.ToTensor(),\n",
    "    T.Resize([224, 224]),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, transform=None, string=\"train\"):\n",
    "        self.imgs_path = \"/kaggle/input/face-expression-recognition-dataset/images/\" + string + \"/\"\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        self.class_map = {\"angry\": 0, \"disgust\": 1, \"fear\": 2, \"happy\": 3, \"neutral\": 4, \"sad\": 5, \"surprise\": 6}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        img = Image.open(img_path)\n",
    "        class_id = self.class_map[class_name]\n",
    "        class_id = torch.tensor(class_id)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, class_id\n",
    "\n",
    "def get_model(num_classes=7):\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    in_features = model._fc.in_features\n",
    "    model._fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    return train_acc, train_loss\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    return test_acc, test_loss\n",
    "\n",
    "train_dataset = MyDataset(transform, \"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_dataset = MyDataset(transform, \"validation\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "model = get_model(num_classes=7).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch: {epoch}/{epochs}\")\n",
    "    train_acc, train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    print(\"\\tTraining Loss: {:.4f} | Accuracy: {:.2f}%\".format(train_loss, train_acc))\n",
    "\n",
    "    test_acc, test_loss = test(model, test_loader, criterion, device)\n",
    "    print(\"\\tValidation Loss: {:.4f} | Accuracy: {:.2f}%\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291bbe0c",
   "metadata": {},
   "source": [
    "### Camera Input + Face Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c00f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import normalize\n",
    "from torchvision import transforms as T\n",
    "import PIL.Image as Image\n",
    "import pandas as pd\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = torch.load(\"model_efficient_2.pt\", map_location=torch.device('cpu'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Function to process each frame and detect emotions\n",
    "def detect_emotion(face_image):\n",
    "    # Convert NumPy array (OpenCV image) to PIL Image\n",
    "    pil_image = Image.fromarray(face_image)\n",
    "    \n",
    "    # Pre-process the face image\n",
    "    transform = T.Compose([\n",
    "        T.Grayscale(num_output_channels=3),\n",
    "        T.Resize([224, 224]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image_tensor = transform(pil_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict emotion\n",
    "    with torch.no_grad():\n",
    "        outputs = F.softmax(model(image_tensor), dim=1)\n",
    "    \n",
    "    probabilities, predicted = torch.max(outputs, 1)\n",
    "    emotion = classes[predicted.item()]\n",
    "    \n",
    "    return emotion, probabilities.item()\n",
    "\n",
    "# Load the pre-defined class labels\n",
    "classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "\n",
    "# Initialize variables for storing detected emotions\n",
    "detected_emotions = []\n",
    "\n",
    "# Open the built-in camera\n",
    "cap = cv2.VideoCapture(0)  # Use default camera (0)\n",
    "\n",
    "# Get the camera frame properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = 30  # Adjust according to your camera\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "output_video_path = \"output_video.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initialize face detector\n",
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Process each frame from the camera\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_detector.detectMultiScale(gray_frame, 1.1, 4)\n",
    "    \n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region from the frame\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Detect emotion for the face image\n",
    "        emotion, probability = detect_emotion(face_image)\n",
    "        \n",
    "        # Display emotion and probability in the corner of the face bounding box\n",
    "        cv2.putText(frame, f\"{emotion} - {probability:.2f}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Write the frame into the output video\n",
    "    out.write(frame)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and writer\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad30613",
   "metadata": {},
   "source": [
    "### Video Input + Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96af86d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotions:\n",
      "neutral\n",
      "neutral\n",
      "sad\n",
      "sad\n",
      "neutral\n",
      "neutral\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "sad\n",
      "fear\n",
      "sad\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "fear\n",
      "sad\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "surprise\n",
      "fear\n",
      "sad\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "sad\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "sad\n",
      "sad\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "fear\n",
      "sad\n",
      "fear\n",
      "sad\n",
      "sad\n",
      "sad\n",
      "Predominant emotion: fear\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import normalize\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = torch.load(\"model_efficient_2.pt\", map_location=torch.device('cpu'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the pre-defined class labels\n",
    "classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "\n",
    "def detect_emotion(face_image):\n",
    "    # Convert NumPy array (OpenCV image) to PIL Image\n",
    "    pil_image = Image.fromarray(face_image)\n",
    "    \n",
    "    # Pre-process the face image\n",
    "    transform = T.Compose([\n",
    "        T.Grayscale(num_output_channels=3),\n",
    "        T.Resize([224, 224]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image_tensor = transform(pil_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict emotion\n",
    "    with torch.no_grad():\n",
    "        outputs = F.softmax(model(image_tensor), dim=1)\n",
    "    \n",
    "    probabilities, predicted = torch.max(outputs, 1)\n",
    "    emotion = classes[predicted.item()]\n",
    "    \n",
    "    return emotion, probabilities.item()\n",
    "\n",
    "# Initialize variables for storing detected emotions\n",
    "detected_emotions = []\n",
    "\n",
    "# Initialize face detector\n",
    "face_detector1 = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open the input video\n",
    "input_video_path = \"Angry_Video.mp4\"\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "output_video_path = \"output_video.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps*8, (frame_width, frame_height))\n",
    "\n",
    "# Process each frame of the video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_detector1.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Extract the face region from the frame\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Detect emotion for the face image\n",
    "        emotion, probability = detect_emotion(face_image)\n",
    "        \n",
    "        # Display emotion and probability in the corner of the face bounding box\n",
    "        cv2.putText(frame, f\"{emotion} - {probability:.2f}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Store detected emotions\n",
    "        detected_emotions.append(emotion)\n",
    "    \n",
    "    # Write the frame into the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and writer\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Display detected emotions at the end of the video\n",
    "print(\"Detected Emotions:\")\n",
    "for emotion in detected_emotions:\n",
    "    print(emotion)\n",
    "\n",
    "# Print the most occurred emotion\n",
    "most_occurred_emotion = max(set(detected_emotions), key=detected_emotions.count)\n",
    "print(\"Predominant emotion:\", most_occurred_emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108311a8",
   "metadata": {},
   "source": [
    "### Image Input + Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9456b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output image saved at output_image.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import normalize\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = torch.load(\"model_efficient_2.pt\", map_location=torch.device('cpu'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the pre-defined class labels\n",
    "classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "\n",
    "# Function to detect emotion in the input image\n",
    "def detect_emotion(image_path, output_path):\n",
    "    # Load the input image\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert the image to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    faces = face_detector.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region from the image\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Pre-process the face image\n",
    "        transform = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Grayscale(num_output_channels=3),\n",
    "            T.Resize([224, 224]),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        image_tensor = transform(face_image).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Predict emotion\n",
    "        with torch.no_grad():\n",
    "            outputs = F.softmax(model(image_tensor), dim=1)\n",
    "        \n",
    "        probabilities, predicted = torch.max(outputs, 1)\n",
    "        emotion = classes[predicted.item()]\n",
    "        \n",
    "        # Display emotion and probability on the image\n",
    "        cv2.putText(frame, f\"{emotion} - {probabilities.item():.2f}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Save the output image\n",
    "    cv2.imwrite(output_path, frame)\n",
    "    print(f\"Output image saved at {output_path}\")\n",
    "\n",
    "# Input image path\n",
    "input_image_path = \"Harry_sad.jpg\"\n",
    "\n",
    "# Output image path\n",
    "output_image_path = \"output_image.jpg\"\n",
    "\n",
    "# Detect emotion in the input image and save the output image\n",
    "detect_emotion(input_image_path, output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c6ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
